\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{cases}
\usepackage{enumerate}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{xfrac}
\usepackage{amsfonts}
\usepackage{float}
\usepackage[margin=60pt]{geometry}


\author{Latrille Thibault\\
\small thibault.latrille@ens-lyon.fr\\[-0.8ex]}
\title{Relatedness and population dynamic.}  

\newcommand{\ud}{\mathrm{d}}

\begin{document}
\maketitle

Relatedness is a key component of inclusive fitness, it is a measure of how closely two individuals are related. The relatedness is highly dependent on the genealogy of the population and of recent coalescent, thus population dynamic drive the evolution of relatedness. We here derive analytic formula of relatedness under several models of population dynamic. 
\section{Distribution of population size during exponential growth.}

In this section, we seek to find the distribution of population size of an exponentially growing population of bacteria, for every time point.
 \paragraph{Notation} $ $\\
 $\bullet \quad t \in \mathbb{R}_+$ is the time variable along the growth\\
 $\bullet \quad N(t)$ is a random variable denoting the size of the population at time $t$\\
 $\bullet \quad P_n(t)$ is the probability mass function of $N(t)$, that is to say $P_n(t)=P(N(t)=n)$\\
 $\bullet \quad r$ is the initial population size\\
 $\bullet \quad \lambda$ is the infinitesimal birth rate\\
 

 The process described here is a Markov process. We assume that each bacteria has an independent constant birth rate $\lambda$ and that they never die. Biologically,  these assumption reflect the fact that we neglect competition, and also that the probability for a bacteria to divide is independent of its age. The Kolmogorov forward equations describing the population size distribution are:   
 \begin{subequations}
  \begin{numcases}{}
    \sum_{n \geq r} P_n(t)=1\\
    P_{r}(0)=1 \\
    P'_n(t)=(n-1)\lambda P_{n-1}(t)-n\lambda P_n(t)\text{ for } n \geq r. \label{eq:dif}
  \end{numcases}
 \end{subequations}
We then multiply the differential equation \eqref{eq:dif} by $(s^1,s^2,s^3,\hdots)$ and add. We obtain a single partial differential equation and the boundary conditions:
 \begin{subequations}
  \begin{numcases}{}
    		G(s,0)=s^{r} \\
    		G(1,t)=1 \\
    		G(0,t)=0 \\
    		\dfrac{\partial G_{N(t)}(s)}{\partial t} = \lambda s(s-1) \dfrac{\partial G_{N(t)}(s)}{\partial s} \text{ for } s \in [0,1], \label{eq:dif:partial}
 \end{numcases}
 \end{subequations}
 where $\displaystyle G_{N(t)}(s)=\sum_{i=1}^{\infty} P_i(t)s^i=\mathbb{E}[ s^{N(t)}] $ is the probability generating function of $N(t)$. \\
The probability generating function (pgf) satisfying this equation (\textit{reference needed}) is:

\begin{equation}
G_{N(t)}(s)=\left( \dfrac{s e^{-\lambda t}}{1-s+s e^{-\lambda t}} \right)^r=\left( \dfrac{sp(t)}{1-s[1-p(t)]} \right)^r \text{ where }p(t)=e^{-\lambda t}.
\end{equation}

Thus the distribution of $N(t)$ is of the negative binomial form (\textit{reference needed}), with support on $\llbracket r ,\infty \llbracket$, that is to say: 
\begin{equation}
P_n(t)=P(N(t)=n)=\binom{n-1}{r-1} p(t)^r [1-p(t)]^{n-r} \text{ for } n \geq r.
\end{equation}

 
 \section{Identity by descendant during exponential growth.}
 In this section, we deal with several exponentially growing families. We assume that families are independent of one another and that each family has the same growth rate but there initial size can differ. We derive the probability of identity (PI), \textit{i.e.} the probability that two randomly chosen are picked from the same family. We used two different methods to derive the PI, the first method rely on the explicit formulation of the distribution, the second one relies solely on the pgf.
 \\
  \paragraph{Notation} $ $\\
 $\bullet \quad d>1$ is the number of families\\
 $\bullet \quad r_i $ for $1 \leq i \leq d$ is the initial size of the $i$\textsuperscript{th} family\\
 $\displaystyle \bullet \quad r_+=\sum_{i=0}^d r_i$ is the initial size of the population \\
 $\bullet \quad N_i(t) $ is the random variable denoting the size of the $i$\textsuperscript{th} family at time $t$ \\
 $\displaystyle \bullet \quad N_+(t)=\sum_{i=0}^d N_i(t)$ is the random variable denoting the size of the population at time $t$ \\
 $\bullet \quad p(t)=e^{-\lambda t} $ as in the previous section\\
 
 \subsection{First method, using distributions.}
  To evaluate the PI, we first need to compute the distribution of size of one single family, conditional on the size of the total population. This conditional distribution is then used to evaluate the PI.
 Let us denote $X_i(t)=N_i(t)-r_i$, then the number of births. $X_i$ has support on $\mathbb{N}$ and follows a negative binomial distributions with parameters $r_i$ and $p(t)$:

\begin{equation}
X_i \sim \textrm{NB}(r_i,p(t)) \iff P(X_i(t)=x)=\binom{x+r_i-1}{x} p(t)^{r_i} [1-p(t)]^{x} \text{ for } x \geq 0.
\end{equation}

This second form of the negative binomial is used for convenience since calculus are more tractable and direct in this way.\\


Let $X_i(t) \sim \textrm{NB}(r_i,p(t))$ for $1 \leq i \leq d$, and denote $X_+(t)=\sum_{i=0}^d X_i(t)$. The sum of independent negative binomials is a negative binomial, since the product of the pgf lead to the same formula:

\begin{equation}
 X_+(t)  \sim \textrm{NB} \left( r_+, p(t) \right). \label{sumNB}
\end{equation}


Let $(x,x_+) \in \mathbb{N}_+^2$ such that $0 \leq x \leq x_+$. And denote $ \displaystyle r_{-i}=\sum_{j \neq i} r_j=r_+-r_i$, then

\begin{align}
P( X_i(t)=x \vert X_+(t)=x_+ ) &=\dfrac{P\left( X_i(t)=x \cap \sum_{j \neq i} X_i(t)=x_+-x \right)}{P\left(X_+(t)=x_+\right)} \text{ by Bayes formula}\\
 &=\dfrac{P\left(X_i(t)=x\right) P\left(\sum_{j \neq i} X_j(t)=x_+-x\right)}{P\left(X_+(t)=x_+\right)} \text{ by independence}\\
 &=\dfrac{\displaystyle \binom{x+r_i-1}{x} p(t)^{r_i} [1-p(t)]^{x} \binom{x_+-x+r_{-i}-1}{x_+-x} p(t)^{r_{-i}} [1-p(t)]^{x_+-x} }{\displaystyle \binom{x_++r_+ -1}{x_+} p(t)^{r_+ } [1-p(t)]^{x_+}} \text{ by \eqref{sumNB}} \\
 &=\dfrac{\displaystyle \binom{x+r_i-1}{x} \binom{x_+-x+r_{-i}-1}{x_+-x}}{\displaystyle \binom{x_+ +r_+ -1}{x_+}}.
\end{align}
 
 Thus the distribution of $X_i(t)$ conditional on $ X_+(t)=x_+$ is of the negative hypergeometric form (\textit{reference needed}) and is independent of $p(t)$.
 
 Computation of the first and second moments is straightforward (\textit{reference needed}): 
 
\begin{align}
\mathbb{E} [ X_i(t) \vert X_+(t)=x_+ ] &=\dfrac{r_i x_+}{r_+ } \\
\mathbb{E} [ X_i(t)^2 \vert X_+(t)=x_+ ] &=\dfrac{r_i x_+ (r_{-i} +x_+ (r_i+1))}{r_+ (1+r_+ )}.
\end{align}

Leading to expectation for $X_i(t)(X_i(t)-1)$ conditional on $X_+(t)$:
\begin{equation}
 \mathbb{E} [ X_i(t)(X_i(t)-1) \vert X_+(t)=x_+ ] =\dfrac{r_i(r_i+1) x_+ ( x_+ -1 ) }{r_+ (r_+ +1 )}. \label{condexpX}
\end{equation}

Using \eqref{condexpX} and the relation $N_i(t)=X_i(t)+r_i$ we derive the expectation for $N_i(t)(N_i(t)-1)$ conditional on $N_+(t)$:
\begin{align}
 \mathbb{E} [ N_i(t)(N_i(t)-1) \vert N_+(t)=n_+ ] &= \mathbb{E} [ ( X_i(t)+r_i)(X_i(t)+r_i -1) \vert X_+(t)+ r_+ = n_+ ] \\
 &= \mathbb{E} [r_i(r_i-1) + 2r_i X_i(t) + X_i(t)(X_i(t)-1) \vert X_+(t)=n_+ - r_+ ]\\
 &= r_i(r_i-1) + 2 r_i \mathbb{E} [ X_i(t) \vert X_+(t)=n_+ - r_+ ]\\
 & \qquad + \mathbb{E} [X_i(t)(X_i(t)-1) \vert X_+(t)=n_+ - r_+ ]\\
  &= r_i(r_i-1) + 2 r_i \dfrac{r_i (n_+ - r_+)}{r_+ } + \dfrac{r_i(r_i+1) (n_+ - r_+) ( n_+ - r_+ -1 ) }{r_+ (r_+ +1 )}\\
 &=\dfrac{r_i(r_i+1) n_+ ( n_+ -1 ) }{r_+ (r_+ +1)} -\dfrac{2 r_{i} (r_+ - r_i) n_+ }{r_+ (r_+ +1)}. \label{condexpN}
\end{align}
 
Leading to the PI of family $i$ conditional on $N_+(t)$, \textit{i.e.} the probability that two bacteria are from family $i$ given the size of the population. 
\begin{align}
  \mathbb{E}\left[ \left. \dfrac{N_i(t)(N_i(t)-1)}{N_+(t)( N_+(t)-1 ) } \right\vert N_+(t)=n_+ \right] &= 
 \dfrac{\mathbb{E}[ N_i(t)(N_i(t)-1) \vert N_+(t)=n_+] }{n_+ (n_+ -1 ) }  \\
 &=\dfrac{r_i(r_i+1)}{r_+ (r_+ +1)} \dfrac{n_+ (n_+ -1 ) }{ n_+ (n_+ -1 ) }- \dfrac{2 r_i (r_+ -r_{i})}{r_+ (r_+ +1)} \dfrac{n_+}{ n_+ (n_+ -1 ) } \text{ by \eqref{condexpN}}\\
 &=\dfrac{r_i(r_i+1)}{r_+ (r_+ +1)}- \dfrac{2 r_i (r_+ -r_{i})}{r_+ (r_+ +1)} \dfrac{1}{ n_+ -1  }. \label{RelatednesscondexpN}
\end{align}

By taking expectation of the PI of family $i$ conditional on $N_+(t)$, over the distribution of $N_+(t)$, we get the PI of family $i$:

\begin{align}
\mathbb{E}\left[ \dfrac{N_i(t)(N_i(t)-1)}{N_+(t) ( N_+(t)-1 )} \right] &= 
 \mathbb{E}\left[ \mathbb{E}\left[ \left. \dfrac{N_i(t)(N_i(t)-1)}{N_+(t)( N_+(t)-1 ) } \right\vert N_+(t) \right] \right]\\
 &=\mathbb{E}\left[\dfrac{r_i(r_i+1)}{r_+ (r_+ +1)}- \dfrac{2 r_i (r_+ -r_{i})}{r_+ (r_+ +1)} \dfrac{1}{N_+(t)-1) } \right] \text{ by \eqref{condexpN}}\\
 &=\dfrac{r_i(r_i+1)}{r_+ (r_+ +1 )}-\dfrac{2 r_i (r_+ -r_{i})}{r_+ (r_+ +1 )}\mathbb{E}\left[\dfrac{1}{N_+(t)-1} \right].
\end{align}

Since $N_+(t) \sim \textrm{NB} (r_+, p(t))$ we can evaluate the expectation $\mathbb{E}\left[\dfrac{1}{N_+(t)-1} \right]$:

\begin{align}
\mathbb{E}\left[\dfrac{1}{N_+(t)-1} \right] &= \sum_{x=r_+}^{\infty } \dfrac{1}{x-1} \binom{x-1}{r_+-1} p(t)^{r_+} [1-p(t)]^{x-r_+} \\
 &=\sum_{y=r_+-1}^{\infty} \dfrac{1}{y} \binom{y}{r_+-1} p(t)^{r_+} [1-p(t)]^{y+1-r_+} \text{ with }y=x-1\\
 &=\dfrac{p(t)}{r_+-1}\sum_{y=r_+-1}^{\infty}\binom{y-1}{(r_+-1)-1} p(t)^{r_+-1} [1-p(t)]^{y-(r_+-1)} \\
 &=\dfrac{p(t)}{r_+-1}.
\end{align}

Thus the probability that two bacteria are from family $i$ is:
\begin{align}
\mathbb{E}\left[ \dfrac{N_i(t)(N_i(t)-1)}{N_+(t) ( N_+(t)-1 )} \right] &= \mathbb{E}\left[\dfrac{1}{N_+(t)-1} \right]\\
 &=\dfrac{r_i(r_i+1)}{r_+ (r_+ +1 )}-\dfrac{2 r_i (r_+ -r_{i})}{r_+ (r_+ +1 )}\dfrac{p(t)}{r_+-1}\\
 &=\dfrac{r_i(r_i+1)}{r_+ (r_+ +1 )}-\dfrac{2 r_i (r_+ -r_{i})p(t)}{r_+ (r_+^2 -1 )}. \label{Relatedness}
\end{align}


By summing \eqref{Relatedness} over all families, we evaluate the overall probability that two bacteria are from the same family:
\begin{align}
\mathbb{E}\left[ \displaystyle \sum_{i=0}^d \dfrac{N_i(t)(N_i(t)-1)}{N_+(t)(N_+(t)-1)} \right] &= \displaystyle \sum_{i=0}^d \mathbb{E}\left[  \dfrac{N_i(t)(N_i(t)-1)}{N_+(t)( N_+(t)-1 ) } \right]\\
&= \displaystyle \sum_{i=0}^d \left[ \dfrac{r_i(r_i+1)}{r_+ (r_+ +1 )}-\dfrac{2 r_i (r_+ -r_{i})p(t)}{r_+ (r_+^2 -1 )} \right] \\
&=    \dfrac{ r_+ + \sum_{i=0}^d r_i^2}{r_+ (r_+ +1)}  -2 e^{-\lambda t} \dfrac{ r_+^2-\sum_{i=0}^d r_i^2}{r_+ (r_+^2 -1) }.
\end{align}

If we assume $r_i=r$ for $0 \leq i \leq d$, the previous formula reduces to:
\begin{align}
\mathbb{E}\left[ \displaystyle \sum_{i=0}^d \dfrac{N_i(t)(N_i(t)-1)}{N_+(t)\left(N_+(t)-1 \right)} \right] &=
 \dfrac{ rd + r^2 d}{rd (rd +1)}  -2 e^{-\lambda t} \dfrac{ (rd)^2-r^2d}{rd [(rd)^2 -1] }\\
 &= \dfrac{r+1}{rd+1}  -2e^{-\lambda t} \dfrac{r(d-1)}{(rd)^2 -1 }.
\end{align}

Which equal $0$ at $t=0$ if $r=1$.

 \subsection{Second method, using probability generating functions.}
 We seek to derive the probability of identity directly as function of the joint pgf of $N_i(t)$ and $N_+(t)$.
 
 Let $G_{N_i(t) ,N_+(t)}(s_i,s_+)$ be the pgf of the joint distribution of $N_i(t)$ and $N_+(t)$. We have the following relation:
 
 \begin{align}
 \displaystyle \int_0^1 \int_0^y \dfrac{\partial^2 }{\partial s_i^2} & \left( \dfrac{G_{N_i(t) ,N_+(t)}(s_i,s_+)}{s_+^2} \right)_{s_i =1} \ud s_+ \ud y \\
 &= \int_0^1 \int_0^y \dfrac{\partial^2 }{\partial s_i^2} \left( \sum_{n_i,n_+ \geq 1} P(N_i(t)=n_i \cap N_+(t)=n_+) s_+^{n_+ -2} s_1^{n_1} \right)_{s_i =1} \ud s_+ \ud y \\ 
 &= \sum_{n_i,n_+ \geq 1} P(N_i(t)=n_i \cap N_+(t)=n_+) \left.  \dfrac{\partial^2 s_i^{n_i}}{\partial s_i^2} \right\vert_{s_i=1} \int_0^1 \int_0^y s_+^{n_+-2} \ud s_+ \ud y\\
 &= \sum_{n_i  ,n_+ \geq 1} P(N_i(t)=n_i \cap N_+(t)=n_+) \dfrac{n_i(n_i-1)}{n_+(n_+-1)}\\
 &= \mathbb{E}\left[ \dfrac{N_i(t)(N_i(t)-1)}{N_+(t) ( N_+(t)-1 )} \right].
 \end{align}
 
 Moreover $G_{N_i(t) ,N_+(t)}(s_i,s_+)$ can be reduced to the product of two pgf: 
  \begin{align}
G_{N_i(t) ,N_+(t)}(s_i,s_+,t) &= \mathbb{E} [ s_i^{N_i(t)} s_+^{N_+(t)}] \\
		&= \mathbb{E} [ s_i^{N_i(t)} s_+^{N_i(t)+N_{-i}(t)}]\\
		&= \mathbb{E} [ (s_i s_+)^{N_i(t)} s_+^{N_{-i}(t)}] \\
		&= \mathbb{E} [ (s_i s_+)^{N_i(t)} ]\mathbb{E} [ s_+^{N_{-i}(t)}] \\
		&= G_{N_i(t)}(s_i s_+) G_{N_{-i}(t)}(s_+).
 \end{align}
 
 Hence, we have 
   \begin{align}
\displaystyle \int_0^1 \int_0^y \dfrac{\partial^2 }{\partial s_i^2} \left( \dfrac{G_{N_i(t) ,N_+(t)}(s_i,s_+)}{s_+^2} \right)_{s_i =1} \ud s_+ \ud y
 &= \int_0^1 \int_0^y \dfrac{\partial^2 }{\partial s_i^2} \left( \dfrac{ G_{N_i(t)}(s_i s_+) G_{N_{-i}(t)}(s_+)}{s_+^2} \right)_{s_i =1} \ud s_+ \ud y \\
  &= \int_0^1 \int_0^y \dfrac{G_{N_{-i}(t)}(s_+)}{s_+^2} \left. \dfrac{\partial^2  G_{N_i(t)}(s_i s_+)  }{\partial s_i^2} \right\vert_{s_i =1} \ud s_+ \ud y \\
    &= \int_0^1 \int_0^y G_{N_{-i}(t)}(s_+) \dfrac{\partial^2  G_{N_i(t)}(s_+)  }{\partial s_+^2} \ud s_+ \ud y.
 \end{align}
 
 Thus the PI is a function solely of $G_{N_{i}(t)}$ and $G_{N_{-i}(t)}$:
 \begin{equation}
 \displaystyle \mathbb{E}\left[ \dfrac{N_i(t)(N_i(t)-1)}{N_+(t) ( N_+(t)-1 )} \right]= \int_0^1 \int_0^y G_{N_{-i}(t)}(s) \dfrac{\partial^2 G_{N_i(t)}(s)}{\partial s^2} \ud s \ud y.
 \end{equation}

This can be evaluated with Mathematica and it gives the same result as previously.


\section{Exponential growth of bacteria, birth-death process.}

In this section we extend the pure birth process to a birth-death process, adding linear a death process.
 \paragraph{Notation} $ $\\
 $\bullet \quad t \in \mathbb{R}_+$, the time variable along the growth\\
 $\bullet \quad P_n(t)$, the probability that the population at time $t$ is of size $n$.\\
 $\bullet \quad N(t)$, the random variable with law $P(N(t)=n)=P_n(t)$\\
 $\bullet \quad r$ the initial population size\\
 $\bullet \quad \lambda$ the infinitesimal birth rate\\
 $\bullet \quad \mu$ the infinitesimal death rate\\
 
 
 The process is such that each bacteria has an independent birth rate $\lambda$ and a death rate $\mu$. Thus the forward equation is:   
 \begin{subequations}
  \begin{numcases}{}
    \sum_{i \geq 1} P_n(t)=1\\
    P_{r}(0)=1 \\
    P'_i(t)=\lambda (i-1) P_{i-1}(t)+\mu (i+1)P_{i+1}(t)-i(\lambda+\mu)P_i(t)\text{ for } i\geq 1 \label{eq:dif}
  \end{numcases}
 \end{subequations}
Multiply the differential equation \eqref{eq:dif} by $(s^1,s^2,s^3,\hdots)$ and add. We obtain a single partial differential equation:
 \begin{subequations}
  \begin{numcases}{}
    		G(s,0)=s^{r} \\
    		G(1,t)=1 \\
    		G(0,t)=0 \\
    		\dfrac{\partial G(s,t)}{\partial t} = (\lambda s -\mu)(s-1) \dfrac{\partial G(s,t)}{\partial s} \text{ for } s \in [0,1] \label{eq:dif:partial:birthdeath}
 \end{numcases}
 \end{subequations}
 with $\displaystyle G(s,t)=\sum_{i=1}^{\infty} P_i(t)s^i=\mathbb{E}[ s^{N(t)}] $ the probability generating function of $N(t)$. \\
The probability generating function satisfying this equation is :

\begin{equation}
\displaystyle  G(s,t)=  \left( \dfrac{\mu (1-s)-(\mu- \lambda s) \exp^{-(\lambda - \mu )t}}{\lambda (1-s)-(\mu -\lambda s) \exp^{-(\lambda -\mu )t}} \right)^{r} \text{ for } s \in [0,1].
\end{equation}
 
\section{Logistic growth of bacteria}

In this section we extend the previous processes to a birth-death process, adding non linear a death process due to competition.

 \paragraph{Notation} $ $\\
 $\bullet \quad t \in \mathbb{R}_+$, the time variable along the growth\\
 $\bullet \quad P_n(t)$, the probability that the population at time $t$ is of size $n$.\\
 $\bullet \quad N(t)$, the random variable with law $P(N(t)=n)=P_n(t)$\\
 $\bullet \quad r$ the initial population size\\
 $\bullet \quad \lambda$ the infinitesimal birth rate\\
 $\bullet \quad c$ the infinitesimal competition rate\\ 
 
 The process is such that each bacteria has an independent birth rate $\lambda$, and the death is only due to competition such that each bacteria dies out a rate $n\lambda$, where $n$ is the number of other bacteria. Thus the forward equation is:   
 \begin{subequations}
  \begin{numcases}{}
    \sum_{i \geq 1} P_n(t)=1\\
    P_{r}(0)=1 \\
    P'_i(t)=(i-1)\lambda P_{i-1}(t)+ci(i+1)P_{i+1}(t)-i(\lambda+c(i-1))P_i(t)\text{ for } i\geq 1 \label{eq:dif}
  \end{numcases}
 \end{subequations}
Multiply the differential equation \eqref{eq:dif} by $(s^1,s^2,s^3,\hdots)$ and add. We obtain a single partial differential equation:
 \begin{subequations}
  \begin{numcases}{}
    		G(s,0)=s^{r} \\
    		G(1,t)=1 \\
    		G(0,t)=0 \\
    		\dfrac{\partial G(s,t)}{\partial t} = \lambda s(s-1) \dfrac{\partial G(s,t)}{\partial s} + c s(1-s) \dfrac{\partial^2 G(s,t)}{\partial s^2} \text{ for } s \in [0,1] \label{eq:dif:partial}
 \end{numcases}
 \end{subequations}
 with $\displaystyle G(s,t)=\sum_{i=1}^{\infty} P_i(t)s^i=\mathbb{E}[ s^{N(t)}] $ the probability generating function of $N(t)$. \\
At equilibrium, with $\rho = \sfrac{\lambda}{c} $, the probability generating function satisfying this equation is :
\begin{equation}
\displaystyle  G(s,\infty)= \sum_{i\geq 1} \dfrac{e^{-\rho}}{1-e^{-\rho}} \dfrac{ \rho^i}{i!} s^i = \dfrac{e^{-\rho}}{1-e^{-\rho}} \left( e^{s \rho} -1 \right) \text{ for } s \in [0,1]
\end{equation}

  And the associated limiting distribution is that of a Poisson variable, conditioned on being positive: 
 \begin{equation}
 \displaystyle P_i(\infty)=\dfrac{e^{-\rho}}{1-e^{-\rho}} \dfrac{ \rho^i}{i!} \text{ for } i\geq 1
 \end{equation}
 
 Denote $G(s,t)=e^{s \sfrac{\lambda}{2c}}K(s,t)$ and the \eqref{eq:dif:partial} become :
 \begin{subequations}
  \begin{numcases}{}
    		K(0,t)=0 \\
    		\dfrac{\partial K(s,t)}{\partial t} = \dfrac{\lambda^2}{2c} s(s-1) K(s,t) + c s(1-s) \dfrac{\partial^2 K(s,t)}{\partial s^2} \text{ for } s \in [0,1] 
 \end{numcases}
 \end{subequations}
 
 
 Multiply the differential equation \eqref{eq:dif} by $(e^{-\theta},e^{-2\theta},e^{-3\theta},\hdots)$ and add. We obtain a single partial differential equation:
  \begin{subequations}
  \begin{numcases}{}
    		H(0,t)=1 \\
    		H(\theta ,0)=e^{-\theta r } \\
    		\dfrac{\partial H(\theta,t)}{\partial t} = \lambda (1-e^{-\theta}) \dfrac{\partial H(\theta,t)}{\partial \theta} + c (e^{\theta}-1) \left( \dfrac{\partial H(\theta,t)}{\partial \theta}+\dfrac{\partial^2 H(\theta,t)}{\partial \theta ^2} \right) \text{ for } \theta \in \mathbb{R_+}
 \end{numcases}
 \end{subequations}
 with $\displaystyle H(s,t)=\sum_{i=1}^{\infty} P_i(t)e^{-i \theta }=\mathbb{E}[ e^{-\theta N(t)}]$ the moment generating function of $N(t)$. \\

\section{biological model}
 \paragraph{Notation} $ $\\
 $\bullet \quad t \in [0, t_{max}]$, the time variable along the growth\\
 $\bullet \quad q=q(t)=P$(coalescence of two lineages | the two lineages were in the same nematode during infection)\\
 $\bullet \quad Q_0^J$ the identity of two bacteria in the same nematode during the free state\\
 $\bullet \quad Q_1^J$ the identity of two bacteria in a different nematode during the free state\\
 $\bullet \quad Q_0(t)$ the identity of two bacteria in the same insect during growth\\
 $\bullet \quad Q_1(t)$ the identity of two bacteria in a different insect during growth\\
 $\bullet \quad I$ the number of insects\\
 $\bullet \quad S$ the number of nematodes\\
 
 \[
  \begin{cases}
    		Q_1(t) &= Q_1^J\\
    		Q_0(t) &= p {\mathbb E}_{Z_1, \hdots, Z_V } \left[ \dfrac{\sum_{i=1}^V Z_i(Z_i-1)}{\sum_{i=1}^V Z_i \left( \sum_{i=1}^V Z_i-1 \right)} \right]\\
    		& \qquad + Q_0^J(1-p) {\mathbb E}_{Z_1, \hdots, Z_V } \left[ \dfrac{\sum_{i=1}^V Z_i(Z_i-1)}{\sum_{i=1}^V Z_i \left( \sum_{i=1}^V Z_i-1 \right)} \right]\\
    		& \qquad + Q_1^J {\mathbb E}_{Z_1, \hdots, Z_V } \left[ \dfrac{\sum_{i=1}^V \sum_{j=1}^{i-1} 2 Z_i Z_j}{\sum_{i=1}^V Z_i \left( \sum_{i=1}^V Z_i-1 \right)} \right]\\
    		Q_0^{J'} &= Q_0(t_{max}) \\
    		Q_1^{J'} &= \dfrac{Q_0(t_{max})}{S} +\dfrac{(S-1) Q_1(t_{max})}{S}
  \end{cases}
  \]
  
  $$\iff$$
  \begin{subequations}
  \begin{numcases}{}
    		Q_1(t) = Q_1^J\\
    		Q_0(t)= q \phi (t) + Q_0^J (1-q) \phi (t) + Q_1^J (1- \phi (t)) \\
    		Q_0^{J'} = Q_0(t_{max}) \\
    		Q_1^{J'} = \dfrac{Q_0(t_{max})}{S} +\dfrac{(S-1) Q_1(t_{max})}{S}
  \end{numcases}
 \end{subequations}
 
 
\end{document}